{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between different models\n",
    "1. Baseline with GPT 3.5 finetuned (ft:gpt-3.5-turbo-1106:ninjatech-ai-dev::8sQyzUKb)\n",
    "2. Baseline with GPT 4\n",
    "3. Prompt2Code with GPT3.5\n",
    "4. Prompt2Code with GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_baseline_with_GPT3_5_finetuned = [\n",
    "    {'success': 8, 'wrong_answer': 38, 'error': 0}\n",
    "]\n",
    "\n",
    "result_baseline_with_GPT4 = [\n",
    "    {'success': 8, 'wrong_answer': 38, 'error': 0}\n",
    "]\n",
    "\n",
    "result_prompt_GPT3_5_reflexion = [\n",
    "    {'success': 30, 'wrong_answer': 16, 'error': 0},\n",
    "    {'success': 28, 'wrong_answer': 18, 'error': 0},\n",
    "    {'success': 33, 'wrong_answer': 13, 'error': 0},\n",
    "    {'success': 33, 'wrong_answer': 13, 'error': 0},\n",
    "    {'success': 27, 'wrong_answer': 19, 'error': 0}\n",
    "]\n",
    "\n",
    "\n",
    "result_prompt_GPT4 = [\n",
    "    {'success': 32, 'wrong_answer': 12, 'error': 2}\n",
    "]\n",
    "\n",
    "\n",
    "# Calculate average success rate and average error rate for each list\n",
    "def calculate_average_rates(result_list):\n",
    "    total_records = len(result_list)\n",
    "    if total_records == 0:\n",
    "        return 0, 0\n",
    "    total_questions = sum(result_list[0].values())\n",
    "    success_sum = sum(result['success'] for result in result_list)\n",
    "    error_sum = sum(result['error'] for result in result_list)\n",
    "    average_success_rate = success_sum / total_records/total_questions\n",
    "    average_exec_rate = 1- error_sum / total_records/total_questions\n",
    "    return average_success_rate, average_exec_rate\n",
    "\n",
    "success_baseline_GPT3_5, execution_baseline_GPT3_5 = calculate_average_rates(result_baseline_with_GPT3_5_finetuned)\n",
    "success_baseline_with_GPT4, execution_baseline_with_GPT4 = calculate_average_rates(result_baseline_with_GPT4)\n",
    "success_code_GPT3_5_reflexion, execution_code_GPT3_5_reflexion = calculate_average_rates(result_prompt_GPT3_5_reflexion)\n",
    "success_code_GPT4, execution_code_GPT4 = calculate_average_rates(result_prompt_GPT4)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fruits = ['Baseline_GPT3_5_finetuned', 'Baseline_GPT4', 'code_GPT3_5_reflexion', 'code_GPT4']\n",
    "counts = [40, 100, 30, 55]\n",
    "bar_labels = ['red', 'blue', '_red', 'orange']\n",
    "bar_colors = ['tab:red', 'tab:blue', 'tab:red', 'tab:orange']\n",
    "\n",
    "ax.bar(fruits, counts, label=bar_labels, color=bar_colors)\n",
    "\n",
    "ax.set_ylabel('fruit supply')\n",
    "ax.set_title('Fruit supply by kind and color')\n",
    "ax.legend(title='Fruit color')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
